{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "---\n",
    "Complete flow of CNN to process an input image and classifies the objects based on values.\n",
    "\n",
    "![CNN Overview](images/cnn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Road Ahead \n",
    "\n",
    "* [Step 1](#Step1): Import Datasets\n",
    "* [Step 2](#Step2): Data Preprocessing\n",
    "* [Step 3](#Step3): Model Architecture\n",
    "* [Step 4](#Step4): Compiling Model\n",
    "* [Step 5](#Step5): Algorithm To Predict Cancer\n",
    "\n",
    "----------------------------------\n",
    "<a id = 'Step1'></a>\n",
    "## Step 1: Import Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import modules that is needed\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to Load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    data_files = np.array(data['filenames'])\n",
    "    data_targets = np_utils.to_categorical(data['target'], 3)\n",
    "    return data_files, data_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files, test_targets = load_dataset('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('train')\n",
    "valid_files, valid_targets = load_dataset('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 3 categories.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6eb68f5bf9f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Statistics about dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'There are total'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'categories.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'There are total'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_files\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'images of skin.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tere are total'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'images for training.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tere are total'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'images for testing.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_files' is not defined"
     ]
    }
   ],
   "source": [
    "# Load list of class name\n",
    "class_names = [item[6:-1] for item in sorted(glob(\"train/*/\"))]\n",
    "\n",
    "# Statistics about dataset\n",
    "print('There are total', len(class_names), 'categories.')\n",
    "print('There are total', len(np.hstack([train_files, test_files, valid_files])), 'images of skin.\\n')\n",
    "print('Tere are total', len(train_files), 'images for training.')\n",
    "print('Tere are total', len(test_files), 'images for testing.')\n",
    "print('Tere are total', len(valid_files), 'images for validation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = 'Step2'></a>\n",
    "## Step 2: Data Preprocessing\n",
    "Conveting Images to RGB matrix.\n",
    "\n",
    "![RGB Image](images/rgb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(299, 299))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (299, 299, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 299, 299, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:34<00:00,  5.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:54<00:00,  3.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [05:46<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [05:31<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"Step3\"></a>\n",
    "## Step 3: Model Architecture\n",
    "\n",
    "CNN model architecture using keras.\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aditya\\Anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Aditya\\Anaconda3\\envs\\py37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 299, 299, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 149, 149, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 149, 149, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 74, 74, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 37, 37, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               20736500  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1503      \n",
      "=================================================================\n",
      "Total params: 20,781,443\n",
      "Trainable params: 20,781,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, \n",
    "                 kernel_size=2, \n",
    "                 padding='same', \n",
    "                 activation='relu',\n",
    "                 input_shape=(299, 299, 3)\n",
    "                ))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,\n",
    "                 kernel_size=2,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                ))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,\n",
    "                 kernel_size=2,\n",
    "                 padding='same',\n",
    "                 activation='relu'\n",
    "                ))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=128,\n",
    "                 kernel_size=2,\n",
    "                 padding='same',\n",
    "                 activation='relu'\n",
    "                ))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Step4\"></a>\n",
    "## Step 4: Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - ETA: 10:09 - loss: 0.7089 - acc: 0.70 - ETA: 5:14 - loss: 0.6853 - acc: 0.7250 - ETA: 3:36 - loss: 0.6180 - acc: 0.766 - ETA: 2:46 - loss: 0.5953 - acc: 0.775 - ETA: 2:16 - loss: 0.6576 - acc: 0.740 - ETA: 1:56 - loss: 0.6306 - acc: 0.758 - ETA: 1:42 - loss: 0.6270 - acc: 0.757 - ETA: 1:31 - loss: 0.6073 - acc: 0.768 - ETA: 1:23 - loss: 0.5986 - acc: 0.777 - ETA: 1:16 - loss: 0.5946 - acc: 0.780 - ETA: 1:11 - loss: 0.6070 - acc: 0.777 - ETA: 1:06 - loss: 0.5973 - acc: 0.779 - ETA: 1:02 - loss: 0.5847 - acc: 0.780 - ETA: 58s - loss: 0.5752 - acc: 0.782 - ETA: 55s - loss: 0.6155 - acc: 0.77 - ETA: 52s - loss: 0.6132 - acc: 0.76 - ETA: 50s - loss: 0.6150 - acc: 0.77 - ETA: 51s - loss: 0.6022 - acc: 0.77 - ETA: 49s - loss: 0.5916 - acc: 0.77 - ETA: 47s - loss: 0.6021 - acc: 0.77 - ETA: 45s - loss: 0.6061 - acc: 0.76 - ETA: 43s - loss: 0.5967 - acc: 0.76 - ETA: 42s - loss: 0.5869 - acc: 0.76 - ETA: 40s - loss: 0.5855 - acc: 0.77 - ETA: 39s - loss: 0.5825 - acc: 0.77 - ETA: 38s - loss: 0.5755 - acc: 0.77 - ETA: 37s - loss: 0.5717 - acc: 0.77 - ETA: 35s - loss: 0.5644 - acc: 0.77 - ETA: 34s - loss: 0.5600 - acc: 0.78 - ETA: 33s - loss: 0.5643 - acc: 0.78 - ETA: 32s - loss: 0.5596 - acc: 0.78 - ETA: 31s - loss: 0.5627 - acc: 0.77 - ETA: 30s - loss: 0.5594 - acc: 0.77 - ETA: 30s - loss: 0.5548 - acc: 0.77 - ETA: 29s - loss: 0.5584 - acc: 0.77 - ETA: 28s - loss: 0.5536 - acc: 0.77 - ETA: 27s - loss: 0.5452 - acc: 0.78 - ETA: 26s - loss: 0.5462 - acc: 0.78 - ETA: 26s - loss: 0.5432 - acc: 0.78 - ETA: 25s - loss: 0.5470 - acc: 0.78 - ETA: 24s - loss: 0.5483 - acc: 0.78 - ETA: 24s - loss: 0.5443 - acc: 0.78 - ETA: 23s - loss: 0.5487 - acc: 0.77 - ETA: 23s - loss: 0.5467 - acc: 0.78 - ETA: 22s - loss: 0.5498 - acc: 0.77 - ETA: 21s - loss: 0.5588 - acc: 0.77 - ETA: 21s - loss: 0.5611 - acc: 0.77 - ETA: 20s - loss: 0.5675 - acc: 0.77 - ETA: 20s - loss: 0.5717 - acc: 0.77 - ETA: 19s - loss: 0.5718 - acc: 0.77 - ETA: 19s - loss: 0.5690 - acc: 0.77 - ETA: 18s - loss: 0.5669 - acc: 0.77 - ETA: 18s - loss: 0.5690 - acc: 0.77 - ETA: 17s - loss: 0.5722 - acc: 0.76 - ETA: 17s - loss: 0.5715 - acc: 0.76 - ETA: 16s - loss: 0.5716 - acc: 0.76 - ETA: 16s - loss: 0.5694 - acc: 0.76 - ETA: 15s - loss: 0.5753 - acc: 0.76 - ETA: 15s - loss: 0.5718 - acc: 0.76 - ETA: 14s - loss: 0.5721 - acc: 0.76 - ETA: 14s - loss: 0.5715 - acc: 0.76 - ETA: 13s - loss: 0.5730 - acc: 0.76 - ETA: 13s - loss: 0.5778 - acc: 0.76 - ETA: 13s - loss: 0.5779 - acc: 0.76 - ETA: 12s - loss: 0.5864 - acc: 0.75 - ETA: 12s - loss: 0.5859 - acc: 0.75 - ETA: 11s - loss: 0.5853 - acc: 0.75 - ETA: 11s - loss: 0.5844 - acc: 0.75 - ETA: 11s - loss: 0.5867 - acc: 0.75 - ETA: 10s - loss: 0.5834 - acc: 0.75 - ETA: 10s - loss: 0.5908 - acc: 0.75 - ETA: 9s - loss: 0.5880 - acc: 0.7563 - ETA: 9s - loss: 0.5873 - acc: 0.756 - ETA: 9s - loss: 0.5898 - acc: 0.754 - ETA: 8s - loss: 0.5871 - acc: 0.756 - ETA: 8s - loss: 0.5858 - acc: 0.757 - ETA: 8s - loss: 0.5827 - acc: 0.758 - ETA: 7s - loss: 0.5869 - acc: 0.758 - ETA: 7s - loss: 0.5850 - acc: 0.759 - ETA: 6s - loss: 0.5821 - acc: 0.760 - ETA: 6s - loss: 0.5821 - acc: 0.759 - ETA: 6s - loss: 0.5823 - acc: 0.759 - ETA: 5s - loss: 0.5831 - acc: 0.758 - ETA: 5s - loss: 0.5819 - acc: 0.758 - ETA: 5s - loss: 0.5815 - acc: 0.758 - ETA: 4s - loss: 0.5845 - acc: 0.757 - ETA: 4s - loss: 0.5844 - acc: 0.756 - ETA: 4s - loss: 0.5874 - acc: 0.756 - ETA: 3s - loss: 0.5860 - acc: 0.756 - ETA: 3s - loss: 0.5873 - acc: 0.755 - ETA: 3s - loss: 0.5897 - acc: 0.754 - ETA: 2s - loss: 0.5896 - acc: 0.754 - ETA: 2s - loss: 0.5922 - acc: 0.754 - ETA: 2s - loss: 0.5946 - acc: 0.752 - ETA: 1s - loss: 0.5944 - acc: 0.753 - ETA: 1s - loss: 0.5934 - acc: 0.753 - ETA: 1s - loss: 0.5930 - acc: 0.753 - ETA: 0s - loss: 0.5905 - acc: 0.754 - ETA: 0s - loss: 0.5957 - acc: 0.754 - 35s 17ms/step - loss: 0.5945 - acc: 0.7550 - val_loss: 0.9915 - val_acc: 0.5933\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.99151, saving model to weights.best.from_scratch.hdf5\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - ETA: 16s - loss: 0.6139 - acc: 0.75 - ETA: 16s - loss: 0.5409 - acc: 0.75 - ETA: 16s - loss: 0.5143 - acc: 0.76 - ETA: 16s - loss: 0.5149 - acc: 0.77 - ETA: 15s - loss: 0.4742 - acc: 0.80 - ETA: 15s - loss: 0.4574 - acc: 0.80 - ETA: 15s - loss: 0.4329 - acc: 0.82 - ETA: 15s - loss: 0.4675 - acc: 0.81 - ETA: 15s - loss: 0.4849 - acc: 0.79 - ETA: 15s - loss: 0.4787 - acc: 0.80 - ETA: 14s - loss: 0.5145 - acc: 0.78 - ETA: 14s - loss: 0.5145 - acc: 0.78 - ETA: 14s - loss: 0.5204 - acc: 0.78 - ETA: 14s - loss: 0.5067 - acc: 0.78 - ETA: 14s - loss: 0.5082 - acc: 0.78 - ETA: 14s - loss: 0.5084 - acc: 0.78 - ETA: 13s - loss: 0.5095 - acc: 0.77 - ETA: 13s - loss: 0.5073 - acc: 0.77 - ETA: 13s - loss: 0.5061 - acc: 0.77 - ETA: 13s - loss: 0.5082 - acc: 0.77 - ETA: 13s - loss: 0.5117 - acc: 0.77 - ETA: 13s - loss: 0.5088 - acc: 0.77 - ETA: 12s - loss: 0.5168 - acc: 0.77 - ETA: 12s - loss: 0.5179 - acc: 0.77 - ETA: 12s - loss: 0.5174 - acc: 0.77 - ETA: 12s - loss: 0.5153 - acc: 0.77 - ETA: 12s - loss: 0.5127 - acc: 0.77 - ETA: 12s - loss: 0.5386 - acc: 0.76 - ETA: 11s - loss: 0.5368 - acc: 0.77 - ETA: 11s - loss: 0.5395 - acc: 0.77 - ETA: 11s - loss: 0.5422 - acc: 0.77 - ETA: 11s - loss: 0.5403 - acc: 0.77 - ETA: 11s - loss: 0.5324 - acc: 0.77 - ETA: 11s - loss: 0.5321 - acc: 0.77 - ETA: 10s - loss: 0.5318 - acc: 0.77 - ETA: 10s - loss: 0.5309 - acc: 0.77 - ETA: 10s - loss: 0.5424 - acc: 0.77 - ETA: 10s - loss: 0.5394 - acc: 0.77 - ETA: 10s - loss: 0.5506 - acc: 0.77 - ETA: 10s - loss: 0.5566 - acc: 0.77 - ETA: 9s - loss: 0.5560 - acc: 0.7720 - ETA: 9s - loss: 0.5585 - acc: 0.772 - ETA: 9s - loss: 0.5584 - acc: 0.770 - ETA: 9s - loss: 0.5561 - acc: 0.771 - ETA: 9s - loss: 0.5546 - acc: 0.773 - ETA: 9s - loss: 0.5505 - acc: 0.773 - ETA: 8s - loss: 0.5492 - acc: 0.773 - ETA: 8s - loss: 0.5521 - acc: 0.771 - ETA: 8s - loss: 0.5508 - acc: 0.772 - ETA: 8s - loss: 0.5474 - acc: 0.771 - ETA: 8s - loss: 0.5530 - acc: 0.769 - ETA: 8s - loss: 0.5543 - acc: 0.768 - ETA: 7s - loss: 0.5531 - acc: 0.768 - ETA: 7s - loss: 0.5523 - acc: 0.771 - ETA: 7s - loss: 0.5548 - acc: 0.771 - ETA: 7s - loss: 0.5550 - acc: 0.771 - ETA: 7s - loss: 0.5534 - acc: 0.772 - ETA: 7s - loss: 0.5472 - acc: 0.775 - ETA: 6s - loss: 0.5453 - acc: 0.776 - ETA: 6s - loss: 0.5519 - acc: 0.775 - ETA: 6s - loss: 0.5529 - acc: 0.774 - ETA: 6s - loss: 0.5528 - acc: 0.773 - ETA: 6s - loss: 0.5528 - acc: 0.774 - ETA: 6s - loss: 0.5522 - acc: 0.775 - ETA: 5s - loss: 0.5515 - acc: 0.776 - ETA: 5s - loss: 0.5496 - acc: 0.777 - ETA: 5s - loss: 0.5482 - acc: 0.778 - ETA: 5s - loss: 0.5470 - acc: 0.777 - ETA: 5s - loss: 0.5442 - acc: 0.778 - ETA: 5s - loss: 0.5446 - acc: 0.777 - ETA: 4s - loss: 0.5462 - acc: 0.776 - ETA: 4s - loss: 0.5462 - acc: 0.774 - ETA: 4s - loss: 0.5431 - acc: 0.775 - ETA: 4s - loss: 0.5438 - acc: 0.775 - ETA: 4s - loss: 0.5413 - acc: 0.776 - ETA: 4s - loss: 0.5413 - acc: 0.775 - ETA: 3s - loss: 0.5424 - acc: 0.774 - ETA: 3s - loss: 0.5429 - acc: 0.772 - ETA: 3s - loss: 0.5445 - acc: 0.769 - ETA: 3s - loss: 0.5421 - acc: 0.771 - ETA: 3s - loss: 0.5484 - acc: 0.769 - ETA: 3s - loss: 0.5515 - acc: 0.767 - ETA: 2s - loss: 0.5545 - acc: 0.766 - ETA: 2s - loss: 0.5536 - acc: 0.767 - ETA: 2s - loss: 0.5523 - acc: 0.768 - ETA: 2s - loss: 0.5524 - acc: 0.767 - ETA: 2s - loss: 0.5537 - acc: 0.767 - ETA: 2s - loss: 0.5536 - acc: 0.767 - ETA: 1s - loss: 0.5525 - acc: 0.768 - ETA: 1s - loss: 0.5532 - acc: 0.768 - ETA: 1s - loss: 0.5527 - acc: 0.769 - ETA: 1s - loss: 0.5523 - acc: 0.769 - ETA: 1s - loss: 0.5543 - acc: 0.767 - ETA: 1s - loss: 0.5562 - acc: 0.767 - ETA: 0s - loss: 0.5566 - acc: 0.766 - ETA: 0s - loss: 0.5549 - acc: 0.767 - ETA: 0s - loss: 0.5561 - acc: 0.767 - ETA: 0s - loss: 0.5572 - acc: 0.767 - ETA: 0s - loss: 0.5558 - acc: 0.768 - 17s 9ms/step - loss: 0.5555 - acc: 0.7685 - val_loss: 1.3534 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.99151\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - ETA: 15s - loss: 0.9202 - acc: 0.70 - ETA: 16s - loss: 0.8002 - acc: 0.65 - ETA: 16s - loss: 0.6668 - acc: 0.75 - ETA: 15s - loss: 0.6794 - acc: 0.71 - ETA: 15s - loss: 0.6256 - acc: 0.75 - ETA: 15s - loss: 0.6200 - acc: 0.74 - ETA: 15s - loss: 0.5834 - acc: 0.75 - ETA: 15s - loss: 0.5802 - acc: 0.76 - ETA: 15s - loss: 0.5573 - acc: 0.78 - ETA: 14s - loss: 0.5362 - acc: 0.80 - ETA: 14s - loss: 0.5164 - acc: 0.80 - ETA: 14s - loss: 0.5076 - acc: 0.79 - ETA: 14s - loss: 0.5070 - acc: 0.79 - ETA: 14s - loss: 0.5145 - acc: 0.78 - ETA: 14s - loss: 0.5031 - acc: 0.79 - ETA: 13s - loss: 0.5096 - acc: 0.79 - ETA: 13s - loss: 0.5158 - acc: 0.79 - ETA: 13s - loss: 0.5013 - acc: 0.79 - ETA: 13s - loss: 0.4954 - acc: 0.79 - ETA: 13s - loss: 0.5007 - acc: 0.78 - ETA: 13s - loss: 0.5267 - acc: 0.78 - ETA: 12s - loss: 0.5303 - acc: 0.77 - ETA: 12s - loss: 0.5315 - acc: 0.77 - ETA: 12s - loss: 0.5345 - acc: 0.77 - ETA: 12s - loss: 0.5302 - acc: 0.77 - ETA: 12s - loss: 0.5305 - acc: 0.77 - ETA: 12s - loss: 0.5274 - acc: 0.77 - ETA: 11s - loss: 0.5224 - acc: 0.77 - ETA: 11s - loss: 0.5232 - acc: 0.77 - ETA: 11s - loss: 0.5207 - acc: 0.77 - ETA: 11s - loss: 0.5266 - acc: 0.77 - ETA: 11s - loss: 0.5316 - acc: 0.77 - ETA: 11s - loss: 0.5269 - acc: 0.77 - ETA: 10s - loss: 0.5226 - acc: 0.77 - ETA: 10s - loss: 0.5214 - acc: 0.77 - ETA: 10s - loss: 0.5184 - acc: 0.77 - ETA: 10s - loss: 0.5260 - acc: 0.77 - ETA: 10s - loss: 0.5315 - acc: 0.77 - ETA: 10s - loss: 0.5320 - acc: 0.77 - ETA: 9s - loss: 0.5273 - acc: 0.7763 - ETA: 9s - loss: 0.5269 - acc: 0.776 - ETA: 9s - loss: 0.5301 - acc: 0.775 - ETA: 9s - loss: 0.5285 - acc: 0.775 - ETA: 9s - loss: 0.5254 - acc: 0.776 - ETA: 9s - loss: 0.5181 - acc: 0.780 - ETA: 8s - loss: 0.5175 - acc: 0.779 - ETA: 8s - loss: 0.5152 - acc: 0.780 - ETA: 8s - loss: 0.5131 - acc: 0.781 - ETA: 8s - loss: 0.5161 - acc: 0.778 - ETA: 8s - loss: 0.5167 - acc: 0.778 - ETA: 8s - loss: 0.5140 - acc: 0.779 - ETA: 7s - loss: 0.5108 - acc: 0.781 - ETA: 7s - loss: 0.5142 - acc: 0.779 - ETA: 7s - loss: 0.5152 - acc: 0.778 - ETA: 7s - loss: 0.5122 - acc: 0.780 - ETA: 7s - loss: 0.5126 - acc: 0.780 - ETA: 7s - loss: 0.5133 - acc: 0.779 - ETA: 6s - loss: 0.5088 - acc: 0.782 - ETA: 6s - loss: 0.5151 - acc: 0.781 - ETA: 6s - loss: 0.5142 - acc: 0.780 - ETA: 6s - loss: 0.5144 - acc: 0.781 - ETA: 6s - loss: 0.5119 - acc: 0.783 - ETA: 6s - loss: 0.5151 - acc: 0.782 - ETA: 5s - loss: 0.5186 - acc: 0.782 - ETA: 5s - loss: 0.5192 - acc: 0.781 - ETA: 5s - loss: 0.5173 - acc: 0.781 - ETA: 5s - loss: 0.5144 - acc: 0.784 - ETA: 5s - loss: 0.5219 - acc: 0.782 - ETA: 5s - loss: 0.5203 - acc: 0.782 - ETA: 4s - loss: 0.5214 - acc: 0.780 - ETA: 4s - loss: 0.5194 - acc: 0.781 - ETA: 4s - loss: 0.5213 - acc: 0.781 - ETA: 4s - loss: 0.5234 - acc: 0.781 - ETA: 4s - loss: 0.5239 - acc: 0.782 - ETA: 4s - loss: 0.5238 - acc: 0.783 - ETA: 3s - loss: 0.5216 - acc: 0.784 - ETA: 3s - loss: 0.5195 - acc: 0.786 - ETA: 3s - loss: 0.5212 - acc: 0.785 - ETA: 3s - loss: 0.5216 - acc: 0.786 - ETA: 3s - loss: 0.5231 - acc: 0.785 - ETA: 3s - loss: 0.5218 - acc: 0.785 - ETA: 2s - loss: 0.5211 - acc: 0.784 - ETA: 2s - loss: 0.5205 - acc: 0.784 - ETA: 2s - loss: 0.5213 - acc: 0.785 - ETA: 2s - loss: 0.5205 - acc: 0.786 - ETA: 2s - loss: 0.5199 - acc: 0.786 - ETA: 2s - loss: 0.5244 - acc: 0.785 - ETA: 2s - loss: 0.5267 - acc: 0.784 - ETA: 1s - loss: 0.5262 - acc: 0.784 - ETA: 1s - loss: 0.5277 - acc: 0.783 - ETA: 1s - loss: 0.5344 - acc: 0.783 - ETA: 1s - loss: 0.5356 - acc: 0.782 - ETA: 1s - loss: 0.5362 - acc: 0.782 - ETA: 1s - loss: 0.5379 - acc: 0.781 - ETA: 0s - loss: 0.5403 - acc: 0.781 - ETA: 0s - loss: 0.5393 - acc: 0.780 - ETA: 0s - loss: 0.5384 - acc: 0.780 - ETA: 0s - loss: 0.5391 - acc: 0.780 - ETA: 0s - loss: 0.5390 - acc: 0.781 - 17s 9ms/step - loss: 0.5378 - acc: 0.7815 - val_loss: 1.2179 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.99151\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - ETA: 16s - loss: 0.4703 - acc: 0.95 - ETA: 16s - loss: 0.4657 - acc: 0.90 - ETA: 16s - loss: 0.4119 - acc: 0.88 - ETA: 15s - loss: 0.4119 - acc: 0.87 - ETA: 15s - loss: 0.3730 - acc: 0.88 - ETA: 15s - loss: 0.3827 - acc: 0.86 - ETA: 15s - loss: 0.4123 - acc: 0.83 - ETA: 15s - loss: 0.4242 - acc: 0.81 - ETA: 15s - loss: 0.4624 - acc: 0.80 - ETA: 14s - loss: 0.4966 - acc: 0.80 - ETA: 14s - loss: 0.4936 - acc: 0.80 - ETA: 14s - loss: 0.5001 - acc: 0.80 - ETA: 14s - loss: 0.4843 - acc: 0.81 - ETA: 14s - loss: 0.4856 - acc: 0.81 - ETA: 14s - loss: 0.4858 - acc: 0.81 - ETA: 13s - loss: 0.4794 - acc: 0.81 - ETA: 13s - loss: 0.4793 - acc: 0.81 - ETA: 13s - loss: 0.4725 - acc: 0.81 - ETA: 13s - loss: 0.4680 - acc: 0.81 - ETA: 13s - loss: 0.4802 - acc: 0.81 - ETA: 13s - loss: 0.4841 - acc: 0.80 - ETA: 12s - loss: 0.4819 - acc: 0.81 - ETA: 12s - loss: 0.4769 - acc: 0.81 - ETA: 12s - loss: 0.4817 - acc: 0.80 - ETA: 12s - loss: 0.4788 - acc: 0.81 - ETA: 12s - loss: 0.4767 - acc: 0.81 - ETA: 12s - loss: 0.4818 - acc: 0.80 - ETA: 11s - loss: 0.4765 - acc: 0.80 - ETA: 11s - loss: 0.4839 - acc: 0.80 - ETA: 11s - loss: 0.4789 - acc: 0.80 - ETA: 11s - loss: 0.4783 - acc: 0.80 - ETA: 11s - loss: 0.4740 - acc: 0.80 - ETA: 11s - loss: 0.4750 - acc: 0.80 - ETA: 10s - loss: 0.4825 - acc: 0.79 - ETA: 10s - loss: 0.4793 - acc: 0.80 - ETA: 10s - loss: 0.4731 - acc: 0.80 - ETA: 10s - loss: 0.4738 - acc: 0.79 - ETA: 10s - loss: 0.4725 - acc: 0.79 - ETA: 10s - loss: 0.4759 - acc: 0.79 - ETA: 9s - loss: 0.4698 - acc: 0.8000 - ETA: 9s - loss: 0.4774 - acc: 0.798 - ETA: 9s - loss: 0.4806 - acc: 0.800 - ETA: 9s - loss: 0.4759 - acc: 0.802 - ETA: 9s - loss: 0.4708 - acc: 0.805 - ETA: 9s - loss: 0.4667 - acc: 0.808 - ETA: 8s - loss: 0.4685 - acc: 0.808 - ETA: 8s - loss: 0.4707 - acc: 0.808 - ETA: 8s - loss: 0.4697 - acc: 0.808 - ETA: 8s - loss: 0.4709 - acc: 0.808 - ETA: 8s - loss: 0.4753 - acc: 0.805 - ETA: 8s - loss: 0.4744 - acc: 0.805 - ETA: 7s - loss: 0.4772 - acc: 0.804 - ETA: 7s - loss: 0.4773 - acc: 0.805 - ETA: 7s - loss: 0.4799 - acc: 0.803 - ETA: 7s - loss: 0.4804 - acc: 0.804 - ETA: 7s - loss: 0.4869 - acc: 0.804 - ETA: 7s - loss: 0.4856 - acc: 0.805 - ETA: 6s - loss: 0.4892 - acc: 0.802 - ETA: 6s - loss: 0.4879 - acc: 0.802 - ETA: 6s - loss: 0.4878 - acc: 0.801 - ETA: 6s - loss: 0.4843 - acc: 0.803 - ETA: 6s - loss: 0.4893 - acc: 0.802 - ETA: 6s - loss: 0.4913 - acc: 0.801 - ETA: 5s - loss: 0.4979 - acc: 0.800 - ETA: 5s - loss: 0.5004 - acc: 0.798 - ETA: 5s - loss: 0.5004 - acc: 0.798 - ETA: 5s - loss: 0.5022 - acc: 0.797 - ETA: 5s - loss: 0.5007 - acc: 0.800 - ETA: 5s - loss: 0.4977 - acc: 0.802 - ETA: 4s - loss: 0.4961 - acc: 0.803 - ETA: 4s - loss: 0.4942 - acc: 0.804 - ETA: 4s - loss: 0.4961 - acc: 0.804 - ETA: 4s - loss: 0.4973 - acc: 0.804 - ETA: 4s - loss: 0.4974 - acc: 0.804 - ETA: 4s - loss: 0.4969 - acc: 0.805 - ETA: 3s - loss: 0.4968 - acc: 0.805 - ETA: 3s - loss: 0.4930 - acc: 0.807 - ETA: 3s - loss: 0.5052 - acc: 0.807 - ETA: 3s - loss: 0.5079 - acc: 0.806 - ETA: 3s - loss: 0.5114 - acc: 0.805 - ETA: 3s - loss: 0.5133 - acc: 0.804 - ETA: 2s - loss: 0.5117 - acc: 0.806 - ETA: 2s - loss: 0.5111 - acc: 0.806 - ETA: 2s - loss: 0.5095 - acc: 0.807 - ETA: 2s - loss: 0.5075 - acc: 0.807 - ETA: 2s - loss: 0.5081 - acc: 0.805 - ETA: 2s - loss: 0.5067 - acc: 0.806 - ETA: 1s - loss: 0.5105 - acc: 0.805 - ETA: 1s - loss: 0.5111 - acc: 0.803 - ETA: 1s - loss: 0.5114 - acc: 0.803 - ETA: 1s - loss: 0.5119 - acc: 0.802 - ETA: 1s - loss: 0.5125 - acc: 0.801 - ETA: 1s - loss: 0.5105 - acc: 0.802 - ETA: 0s - loss: 0.5103 - acc: 0.801 - ETA: 0s - loss: 0.5127 - acc: 0.800 - ETA: 0s - loss: 0.5108 - acc: 0.801 - ETA: 0s - loss: 0.5105 - acc: 0.801 - ETA: 0s - loss: 0.5076 - acc: 0.802 - ETA: 0s - loss: 0.5118 - acc: 0.800 - 17s 9ms/step - loss: 0.5115 - acc: 0.8000 - val_loss: 1.0607 - val_acc: 0.5933\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.99151\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 16s - loss: 0.4137 - acc: 0.90 - ETA: 16s - loss: 0.4894 - acc: 0.80 - ETA: 16s - loss: 0.4793 - acc: 0.76 - ETA: 16s - loss: 0.4307 - acc: 0.80 - ETA: 16s - loss: 0.4064 - acc: 0.80 - ETA: 15s - loss: 0.4289 - acc: 0.78 - ETA: 15s - loss: 0.4312 - acc: 0.80 - ETA: 15s - loss: 0.4522 - acc: 0.78 - ETA: 15s - loss: 0.4670 - acc: 0.79 - ETA: 15s - loss: 0.4675 - acc: 0.78 - ETA: 14s - loss: 0.4702 - acc: 0.78 - ETA: 14s - loss: 0.4528 - acc: 0.80 - ETA: 14s - loss: 0.4595 - acc: 0.80 - ETA: 14s - loss: 0.4810 - acc: 0.78 - ETA: 14s - loss: 0.4843 - acc: 0.78 - ETA: 14s - loss: 0.4723 - acc: 0.78 - ETA: 13s - loss: 0.4678 - acc: 0.79 - ETA: 13s - loss: 0.4551 - acc: 0.80 - ETA: 13s - loss: 0.4564 - acc: 0.79 - ETA: 13s - loss: 0.4959 - acc: 0.79 - ETA: 13s - loss: 0.4888 - acc: 0.79 - ETA: 13s - loss: 0.5117 - acc: 0.78 - ETA: 12s - loss: 0.5085 - acc: 0.78 - ETA: 12s - loss: 0.5057 - acc: 0.78 - ETA: 12s - loss: 0.5035 - acc: 0.79 - ETA: 12s - loss: 0.5115 - acc: 0.78 - ETA: 12s - loss: 0.5073 - acc: 0.78 - ETA: 12s - loss: 0.5099 - acc: 0.78 - ETA: 11s - loss: 0.5053 - acc: 0.78 - ETA: 11s - loss: 0.5011 - acc: 0.79 - ETA: 11s - loss: 0.4998 - acc: 0.79 - ETA: 11s - loss: 0.4955 - acc: 0.79 - ETA: 11s - loss: 0.4904 - acc: 0.79 - ETA: 11s - loss: 0.4929 - acc: 0.79 - ETA: 10s - loss: 0.4874 - acc: 0.79 - ETA: 10s - loss: 0.4856 - acc: 0.80 - ETA: 10s - loss: 0.4864 - acc: 0.79 - ETA: 10s - loss: 0.4892 - acc: 0.79 - ETA: 10s - loss: 0.4908 - acc: 0.79 - ETA: 10s - loss: 0.4890 - acc: 0.79 - ETA: 9s - loss: 0.4860 - acc: 0.8024 - ETA: 9s - loss: 0.4852 - acc: 0.804 - ETA: 9s - loss: 0.4788 - acc: 0.807 - ETA: 9s - loss: 0.4854 - acc: 0.802 - ETA: 9s - loss: 0.4833 - acc: 0.803 - ETA: 9s - loss: 0.4846 - acc: 0.803 - ETA: 8s - loss: 0.4826 - acc: 0.805 - ETA: 8s - loss: 0.4815 - acc: 0.806 - ETA: 8s - loss: 0.4848 - acc: 0.804 - ETA: 8s - loss: 0.4849 - acc: 0.803 - ETA: 8s - loss: 0.4807 - acc: 0.805 - ETA: 7s - loss: 0.4771 - acc: 0.806 - ETA: 7s - loss: 0.4763 - acc: 0.806 - ETA: 7s - loss: 0.4789 - acc: 0.805 - ETA: 7s - loss: 0.4773 - acc: 0.806 - ETA: 7s - loss: 0.4771 - acc: 0.806 - ETA: 7s - loss: 0.4737 - acc: 0.807 - ETA: 6s - loss: 0.4752 - acc: 0.807 - ETA: 6s - loss: 0.4719 - acc: 0.809 - ETA: 6s - loss: 0.4755 - acc: 0.807 - ETA: 6s - loss: 0.4788 - acc: 0.807 - ETA: 6s - loss: 0.4780 - acc: 0.807 - ETA: 6s - loss: 0.4791 - acc: 0.807 - ETA: 5s - loss: 0.4808 - acc: 0.805 - ETA: 5s - loss: 0.4799 - acc: 0.806 - ETA: 5s - loss: 0.4854 - acc: 0.805 - ETA: 5s - loss: 0.4866 - acc: 0.804 - ETA: 5s - loss: 0.4864 - acc: 0.805 - ETA: 5s - loss: 0.4883 - acc: 0.805 - ETA: 4s - loss: 0.4920 - acc: 0.805 - ETA: 4s - loss: 0.4929 - acc: 0.803 - ETA: 4s - loss: 0.4947 - acc: 0.802 - ETA: 4s - loss: 0.4939 - acc: 0.803 - ETA: 4s - loss: 0.4933 - acc: 0.803 - ETA: 4s - loss: 0.4929 - acc: 0.803 - ETA: 3s - loss: 0.4912 - acc: 0.803 - ETA: 3s - loss: 0.4894 - acc: 0.803 - ETA: 3s - loss: 0.4893 - acc: 0.803 - ETA: 3s - loss: 0.4924 - acc: 0.801 - ETA: 3s - loss: 0.4905 - acc: 0.802 - ETA: 3s - loss: 0.4914 - acc: 0.801 - ETA: 2s - loss: 0.4916 - acc: 0.801 - ETA: 2s - loss: 0.4954 - acc: 0.801 - ETA: 2s - loss: 0.4943 - acc: 0.802 - ETA: 2s - loss: 0.4924 - acc: 0.803 - ETA: 2s - loss: 0.4908 - acc: 0.804 - ETA: 2s - loss: 0.4873 - acc: 0.806 - ETA: 1s - loss: 0.4846 - acc: 0.808 - ETA: 1s - loss: 0.4818 - acc: 0.808 - ETA: 1s - loss: 0.4810 - acc: 0.807 - ETA: 1s - loss: 0.4870 - acc: 0.806 - ETA: 1s - loss: 0.4842 - acc: 0.807 - ETA: 1s - loss: 0.4848 - acc: 0.807 - ETA: 0s - loss: 0.4844 - acc: 0.807 - ETA: 0s - loss: 0.4841 - acc: 0.807 - ETA: 0s - loss: 0.4832 - acc: 0.807 - ETA: 0s - loss: 0.4844 - acc: 0.806 - ETA: 0s - loss: 0.4853 - acc: 0.807 - ETA: 0s - loss: 0.4851 - acc: 0.808 - 17s 9ms/step - loss: 0.4851 - acc: 0.8075 - val_loss: 1.1527 - val_acc: 0.6400\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.99151\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - ETA: 16s - loss: 0.2500 - acc: 0.90 - ETA: 16s - loss: 0.4000 - acc: 0.85 - ETA: 16s - loss: 0.3319 - acc: 0.90 - ETA: 15s - loss: 0.3170 - acc: 0.90 - ETA: 15s - loss: 0.3136 - acc: 0.90 - ETA: 15s - loss: 0.2931 - acc: 0.90 - ETA: 15s - loss: 0.3594 - acc: 0.87 - ETA: 15s - loss: 0.3549 - acc: 0.88 - ETA: 15s - loss: 0.3744 - acc: 0.86 - ETA: 14s - loss: 0.3655 - acc: 0.87 - ETA: 14s - loss: 0.3641 - acc: 0.86 - ETA: 14s - loss: 0.3545 - acc: 0.86 - ETA: 14s - loss: 0.3644 - acc: 0.86 - ETA: 14s - loss: 0.3599 - acc: 0.86 - ETA: 14s - loss: 0.3624 - acc: 0.86 - ETA: 13s - loss: 0.3764 - acc: 0.86 - ETA: 13s - loss: 0.3843 - acc: 0.85 - ETA: 13s - loss: 0.3981 - acc: 0.84 - ETA: 13s - loss: 0.4003 - acc: 0.83 - ETA: 13s - loss: 0.4076 - acc: 0.83 - ETA: 13s - loss: 0.4114 - acc: 0.82 - ETA: 12s - loss: 0.4201 - acc: 0.82 - ETA: 12s - loss: 0.4103 - acc: 0.82 - ETA: 12s - loss: 0.4103 - acc: 0.82 - ETA: 12s - loss: 0.4259 - acc: 0.82 - ETA: 12s - loss: 0.4285 - acc: 0.82 - ETA: 12s - loss: 0.4267 - acc: 0.82 - ETA: 11s - loss: 0.4252 - acc: 0.82 - ETA: 11s - loss: 0.4302 - acc: 0.82 - ETA: 11s - loss: 0.4331 - acc: 0.82 - ETA: 11s - loss: 0.4321 - acc: 0.82 - ETA: 11s - loss: 0.4293 - acc: 0.82 - ETA: 11s - loss: 0.4331 - acc: 0.82 - ETA: 10s - loss: 0.4327 - acc: 0.81 - ETA: 10s - loss: 0.4355 - acc: 0.81 - ETA: 10s - loss: 0.4387 - acc: 0.81 - ETA: 10s - loss: 0.4408 - acc: 0.81 - ETA: 10s - loss: 0.4392 - acc: 0.81 - ETA: 10s - loss: 0.4374 - acc: 0.81 - ETA: 9s - loss: 0.4339 - acc: 0.8162 - ETA: 9s - loss: 0.4356 - acc: 0.814 - ETA: 9s - loss: 0.4287 - acc: 0.819 - ETA: 9s - loss: 0.4258 - acc: 0.819 - ETA: 9s - loss: 0.4238 - acc: 0.821 - ETA: 9s - loss: 0.4238 - acc: 0.821 - ETA: 8s - loss: 0.4293 - acc: 0.821 - ETA: 8s - loss: 0.4257 - acc: 0.823 - ETA: 8s - loss: 0.4258 - acc: 0.822 - ETA: 8s - loss: 0.4384 - acc: 0.819 - ETA: 8s - loss: 0.4377 - acc: 0.820 - ETA: 8s - loss: 0.4429 - acc: 0.818 - ETA: 7s - loss: 0.4435 - acc: 0.818 - ETA: 7s - loss: 0.4451 - acc: 0.817 - ETA: 7s - loss: 0.4466 - acc: 0.815 - ETA: 7s - loss: 0.4443 - acc: 0.817 - ETA: 7s - loss: 0.4387 - acc: 0.820 - ETA: 7s - loss: 0.4427 - acc: 0.820 - ETA: 6s - loss: 0.4424 - acc: 0.821 - ETA: 6s - loss: 0.4391 - acc: 0.822 - ETA: 6s - loss: 0.4440 - acc: 0.821 - ETA: 6s - loss: 0.4427 - acc: 0.823 - ETA: 6s - loss: 0.4427 - acc: 0.822 - ETA: 6s - loss: 0.4434 - acc: 0.822 - ETA: 5s - loss: 0.4462 - acc: 0.821 - ETA: 5s - loss: 0.4466 - acc: 0.821 - ETA: 5s - loss: 0.4463 - acc: 0.822 - ETA: 5s - loss: 0.4434 - acc: 0.823 - ETA: 5s - loss: 0.4416 - acc: 0.823 - ETA: 5s - loss: 0.4404 - acc: 0.823 - ETA: 4s - loss: 0.4404 - acc: 0.824 - ETA: 4s - loss: 0.4408 - acc: 0.823 - ETA: 4s - loss: 0.4372 - acc: 0.825 - ETA: 4s - loss: 0.4379 - acc: 0.825 - ETA: 4s - loss: 0.4426 - acc: 0.823 - ETA: 4s - loss: 0.4428 - acc: 0.822 - ETA: 3s - loss: 0.4464 - acc: 0.819 - ETA: 3s - loss: 0.4454 - acc: 0.820 - ETA: 3s - loss: 0.4427 - acc: 0.822 - ETA: 3s - loss: 0.4433 - acc: 0.822 - ETA: 3s - loss: 0.4409 - acc: 0.823 - ETA: 3s - loss: 0.4428 - acc: 0.822 - ETA: 2s - loss: 0.4397 - acc: 0.825 - ETA: 2s - loss: 0.4401 - acc: 0.824 - ETA: 2s - loss: 0.4416 - acc: 0.823 - ETA: 2s - loss: 0.4449 - acc: 0.821 - ETA: 2s - loss: 0.4430 - acc: 0.822 - ETA: 2s - loss: 0.4425 - acc: 0.821 - ETA: 1s - loss: 0.4437 - acc: 0.821 - ETA: 1s - loss: 0.4455 - acc: 0.822 - ETA: 1s - loss: 0.4439 - acc: 0.822 - ETA: 1s - loss: 0.4425 - acc: 0.823 - ETA: 1s - loss: 0.4457 - acc: 0.822 - ETA: 1s - loss: 0.4497 - acc: 0.820 - ETA: 0s - loss: 0.4488 - acc: 0.820 - ETA: 0s - loss: 0.4460 - acc: 0.822 - ETA: 0s - loss: 0.4449 - acc: 0.823 - ETA: 0s - loss: 0.4449 - acc: 0.822 - ETA: 0s - loss: 0.4441 - acc: 0.822 - ETA: 0s - loss: 0.4437 - acc: 0.822 - 17s 9ms/step - loss: 0.4416 - acc: 0.8235 - val_loss: 1.4069 - val_acc: 0.6267\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99151\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - ETA: 16s - loss: 0.5634 - acc: 0.85 - ETA: 16s - loss: 0.6008 - acc: 0.77 - ETA: 16s - loss: 0.5455 - acc: 0.78 - ETA: 16s - loss: 0.5173 - acc: 0.78 - ETA: 16s - loss: 0.5478 - acc: 0.78 - ETA: 15s - loss: 0.4998 - acc: 0.80 - ETA: 15s - loss: 0.4665 - acc: 0.81 - ETA: 15s - loss: 0.4448 - acc: 0.81 - ETA: 15s - loss: 0.4679 - acc: 0.81 - ETA: 15s - loss: 0.4523 - acc: 0.82 - ETA: 14s - loss: 0.4577 - acc: 0.81 - ETA: 14s - loss: 0.4441 - acc: 0.82 - ETA: 14s - loss: 0.4566 - acc: 0.81 - ETA: 14s - loss: 0.4464 - acc: 0.81 - ETA: 14s - loss: 0.4321 - acc: 0.82 - ETA: 14s - loss: 0.4293 - acc: 0.81 - ETA: 13s - loss: 0.4426 - acc: 0.81 - ETA: 13s - loss: 0.4362 - acc: 0.81 - ETA: 13s - loss: 0.4525 - acc: 0.81 - ETA: 13s - loss: 0.4401 - acc: 0.82 - ETA: 13s - loss: 0.4355 - acc: 0.82 - ETA: 13s - loss: 0.4304 - acc: 0.82 - ETA: 12s - loss: 0.4353 - acc: 0.82 - ETA: 12s - loss: 0.4400 - acc: 0.82 - ETA: 12s - loss: 0.4382 - acc: 0.82 - ETA: 12s - loss: 0.4531 - acc: 0.82 - ETA: 12s - loss: 0.4487 - acc: 0.82 - ETA: 12s - loss: 0.4422 - acc: 0.82 - ETA: 11s - loss: 0.4333 - acc: 0.83 - ETA: 11s - loss: 0.4258 - acc: 0.83 - ETA: 11s - loss: 0.4257 - acc: 0.83 - ETA: 11s - loss: 0.4251 - acc: 0.83 - ETA: 11s - loss: 0.4252 - acc: 0.83 - ETA: 11s - loss: 0.4206 - acc: 0.84 - ETA: 10s - loss: 0.4232 - acc: 0.84 - ETA: 10s - loss: 0.4212 - acc: 0.84 - ETA: 10s - loss: 0.4260 - acc: 0.83 - ETA: 10s - loss: 0.4270 - acc: 0.83 - ETA: 10s - loss: 0.4292 - acc: 0.83 - ETA: 10s - loss: 0.4250 - acc: 0.83 - ETA: 9s - loss: 0.4217 - acc: 0.8378 - ETA: 9s - loss: 0.4255 - acc: 0.838 - ETA: 9s - loss: 0.4224 - acc: 0.839 - ETA: 9s - loss: 0.4187 - acc: 0.840 - ETA: 9s - loss: 0.4155 - acc: 0.842 - ETA: 9s - loss: 0.4203 - acc: 0.840 - ETA: 8s - loss: 0.4245 - acc: 0.838 - ETA: 8s - loss: 0.4253 - acc: 0.837 - ETA: 8s - loss: 0.4233 - acc: 0.838 - ETA: 8s - loss: 0.4233 - acc: 0.839 - ETA: 8s - loss: 0.4188 - acc: 0.842 - ETA: 8s - loss: 0.4222 - acc: 0.841 - ETA: 7s - loss: 0.4262 - acc: 0.840 - ETA: 7s - loss: 0.4232 - acc: 0.841 - ETA: 7s - loss: 0.4192 - acc: 0.843 - ETA: 7s - loss: 0.4363 - acc: 0.839 - ETA: 7s - loss: 0.4381 - acc: 0.838 - ETA: 7s - loss: 0.4414 - acc: 0.835 - ETA: 6s - loss: 0.4431 - acc: 0.834 - ETA: 6s - loss: 0.4432 - acc: 0.834 - ETA: 6s - loss: 0.4413 - acc: 0.835 - ETA: 6s - loss: 0.4452 - acc: 0.833 - ETA: 6s - loss: 0.4476 - acc: 0.832 - ETA: 6s - loss: 0.4442 - acc: 0.834 - ETA: 5s - loss: 0.4409 - acc: 0.836 - ETA: 5s - loss: 0.4380 - acc: 0.836 - ETA: 5s - loss: 0.4404 - acc: 0.835 - ETA: 5s - loss: 0.4393 - acc: 0.835 - ETA: 5s - loss: 0.4380 - acc: 0.836 - ETA: 4s - loss: 0.4421 - acc: 0.835 - ETA: 4s - loss: 0.4452 - acc: 0.833 - ETA: 4s - loss: 0.4481 - acc: 0.830 - ETA: 4s - loss: 0.4485 - acc: 0.830 - ETA: 4s - loss: 0.4467 - acc: 0.830 - ETA: 4s - loss: 0.4503 - acc: 0.828 - ETA: 3s - loss: 0.4497 - acc: 0.828 - ETA: 3s - loss: 0.4482 - acc: 0.829 - ETA: 3s - loss: 0.4462 - acc: 0.830 - ETA: 3s - loss: 0.4502 - acc: 0.829 - ETA: 3s - loss: 0.4482 - acc: 0.829 - ETA: 3s - loss: 0.4481 - acc: 0.829 - ETA: 2s - loss: 0.4466 - acc: 0.830 - ETA: 2s - loss: 0.4462 - acc: 0.829 - ETA: 2s - loss: 0.4481 - acc: 0.828 - ETA: 2s - loss: 0.4463 - acc: 0.829 - ETA: 2s - loss: 0.4448 - acc: 0.831 - ETA: 2s - loss: 0.4419 - acc: 0.832 - ETA: 1s - loss: 0.4447 - acc: 0.831 - ETA: 1s - loss: 0.4458 - acc: 0.829 - ETA: 1s - loss: 0.4443 - acc: 0.830 - ETA: 1s - loss: 0.4453 - acc: 0.830 - ETA: 1s - loss: 0.4484 - acc: 0.829 - ETA: 1s - loss: 0.4479 - acc: 0.830 - ETA: 0s - loss: 0.4459 - acc: 0.830 - ETA: 0s - loss: 0.4451 - acc: 0.831 - ETA: 0s - loss: 0.4433 - acc: 0.832 - ETA: 0s - loss: 0.4422 - acc: 0.832 - ETA: 0s - loss: 0.4426 - acc: 0.832 - ETA: 0s - loss: 0.4413 - acc: 0.833 - 17s 9ms/step - loss: 0.4416 - acc: 0.8330 - val_loss: 1.2035 - val_acc: 0.6067\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99151\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - ETA: 15s - loss: 0.6272 - acc: 0.75 - ETA: 16s - loss: 0.4718 - acc: 0.82 - ETA: 16s - loss: 0.4235 - acc: 0.85 - ETA: 15s - loss: 0.4011 - acc: 0.86 - ETA: 15s - loss: 0.4242 - acc: 0.85 - ETA: 15s - loss: 0.4088 - acc: 0.85 - ETA: 15s - loss: 0.4052 - acc: 0.85 - ETA: 15s - loss: 0.4227 - acc: 0.83 - ETA: 15s - loss: 0.4164 - acc: 0.83 - ETA: 14s - loss: 0.4390 - acc: 0.83 - ETA: 14s - loss: 0.4354 - acc: 0.83 - ETA: 14s - loss: 0.4266 - acc: 0.84 - ETA: 14s - loss: 0.4073 - acc: 0.85 - ETA: 14s - loss: 0.3936 - acc: 0.86 - ETA: 14s - loss: 0.4760 - acc: 0.84 - ETA: 13s - loss: 0.4689 - acc: 0.85 - ETA: 13s - loss: 0.4617 - acc: 0.85 - ETA: 13s - loss: 0.4575 - acc: 0.84 - ETA: 13s - loss: 0.4458 - acc: 0.85 - ETA: 13s - loss: 0.4414 - acc: 0.84 - ETA: 13s - loss: 0.4311 - acc: 0.85 - ETA: 12s - loss: 0.4251 - acc: 0.85 - ETA: 12s - loss: 0.4138 - acc: 0.85 - ETA: 12s - loss: 0.4280 - acc: 0.85 - ETA: 12s - loss: 0.4180 - acc: 0.85 - ETA: 12s - loss: 0.4136 - acc: 0.85 - ETA: 12s - loss: 0.4150 - acc: 0.85 - ETA: 11s - loss: 0.4070 - acc: 0.85 - ETA: 11s - loss: 0.3984 - acc: 0.86 - ETA: 11s - loss: 0.3923 - acc: 0.86 - ETA: 11s - loss: 0.3973 - acc: 0.86 - ETA: 11s - loss: 0.3955 - acc: 0.85 - ETA: 11s - loss: 0.3933 - acc: 0.85 - ETA: 10s - loss: 0.4000 - acc: 0.85 - ETA: 10s - loss: 0.4033 - acc: 0.85 - ETA: 10s - loss: 0.4208 - acc: 0.85 - ETA: 10s - loss: 0.4162 - acc: 0.85 - ETA: 10s - loss: 0.4102 - acc: 0.85 - ETA: 10s - loss: 0.4206 - acc: 0.85 - ETA: 9s - loss: 0.4198 - acc: 0.8562 - ETA: 9s - loss: 0.4165 - acc: 0.857 - ETA: 9s - loss: 0.4115 - acc: 0.857 - ETA: 9s - loss: 0.4152 - acc: 0.855 - ETA: 9s - loss: 0.4155 - acc: 0.856 - ETA: 9s - loss: 0.4118 - acc: 0.857 - ETA: 8s - loss: 0.4083 - acc: 0.859 - ETA: 8s - loss: 0.4037 - acc: 0.861 - ETA: 8s - loss: 0.3990 - acc: 0.863 - ETA: 8s - loss: 0.4061 - acc: 0.863 - ETA: 8s - loss: 0.4015 - acc: 0.865 - ETA: 8s - loss: 0.4023 - acc: 0.864 - ETA: 7s - loss: 0.4011 - acc: 0.863 - ETA: 7s - loss: 0.4152 - acc: 0.863 - ETA: 7s - loss: 0.4127 - acc: 0.863 - ETA: 7s - loss: 0.4130 - acc: 0.861 - ETA: 7s - loss: 0.4133 - acc: 0.859 - ETA: 7s - loss: 0.4113 - acc: 0.859 - ETA: 6s - loss: 0.4093 - acc: 0.860 - ETA: 6s - loss: 0.4106 - acc: 0.856 - ETA: 6s - loss: 0.4097 - acc: 0.857 - ETA: 6s - loss: 0.4050 - acc: 0.859 - ETA: 6s - loss: 0.4036 - acc: 0.858 - ETA: 6s - loss: 0.4026 - acc: 0.858 - ETA: 5s - loss: 0.4049 - acc: 0.857 - ETA: 5s - loss: 0.4029 - acc: 0.856 - ETA: 5s - loss: 0.4029 - acc: 0.855 - ETA: 5s - loss: 0.4007 - acc: 0.855 - ETA: 5s - loss: 0.3991 - acc: 0.855 - ETA: 5s - loss: 0.3996 - acc: 0.854 - ETA: 4s - loss: 0.3968 - acc: 0.855 - ETA: 4s - loss: 0.3949 - acc: 0.856 - ETA: 4s - loss: 0.3926 - acc: 0.856 - ETA: 4s - loss: 0.3927 - acc: 0.856 - ETA: 4s - loss: 0.3953 - acc: 0.856 - ETA: 4s - loss: 0.3957 - acc: 0.856 - ETA: 3s - loss: 0.3939 - acc: 0.857 - ETA: 3s - loss: 0.3964 - acc: 0.857 - ETA: 3s - loss: 0.4034 - acc: 0.853 - ETA: 3s - loss: 0.4107 - acc: 0.853 - ETA: 3s - loss: 0.4110 - acc: 0.853 - ETA: 3s - loss: 0.4103 - acc: 0.853 - ETA: 3s - loss: 0.4099 - acc: 0.853 - ETA: 2s - loss: 0.4112 - acc: 0.852 - ETA: 2s - loss: 0.4098 - acc: 0.853 - ETA: 2s - loss: 0.4090 - acc: 0.852 - ETA: 2s - loss: 0.4086 - acc: 0.853 - ETA: 2s - loss: 0.4074 - acc: 0.854 - ETA: 2s - loss: 0.4067 - acc: 0.855 - ETA: 1s - loss: 0.4107 - acc: 0.853 - ETA: 1s - loss: 0.4111 - acc: 0.853 - ETA: 1s - loss: 0.4102 - acc: 0.853 - ETA: 1s - loss: 0.4092 - acc: 0.853 - ETA: 1s - loss: 0.4090 - acc: 0.853 - ETA: 1s - loss: 0.4083 - acc: 0.853 - ETA: 0s - loss: 0.4085 - acc: 0.853 - ETA: 0s - loss: 0.4093 - acc: 0.853 - ETA: 0s - loss: 0.4096 - acc: 0.853 - ETA: 0s - loss: 0.4095 - acc: 0.854 - ETA: 0s - loss: 0.4084 - acc: 0.854 - 17s 9ms/step - loss: 0.4089 - acc: 0.8540 - val_loss: 1.5576 - val_acc: 0.5867\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99151\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 16s - loss: 0.2413 - acc: 0.90 - ETA: 17s - loss: 0.2918 - acc: 0.87 - ETA: 16s - loss: 0.2740 - acc: 0.86 - ETA: 16s - loss: 0.3195 - acc: 0.85 - ETA: 16s - loss: 0.3250 - acc: 0.86 - ETA: 16s - loss: 0.3519 - acc: 0.84 - ETA: 15s - loss: 0.3832 - acc: 0.84 - ETA: 15s - loss: 0.3809 - acc: 0.83 - ETA: 15s - loss: 0.3795 - acc: 0.83 - ETA: 15s - loss: 0.3767 - acc: 0.84 - ETA: 15s - loss: 0.3707 - acc: 0.84 - ETA: 14s - loss: 0.3519 - acc: 0.85 - ETA: 14s - loss: 0.3495 - acc: 0.85 - ETA: 14s - loss: 0.3591 - acc: 0.85 - ETA: 14s - loss: 0.3595 - acc: 0.84 - ETA: 14s - loss: 0.3520 - acc: 0.85 - ETA: 13s - loss: 0.3391 - acc: 0.86 - ETA: 13s - loss: 0.3376 - acc: 0.86 - ETA: 13s - loss: 0.3291 - acc: 0.86 - ETA: 13s - loss: 0.3205 - acc: 0.87 - ETA: 13s - loss: 0.3232 - acc: 0.86 - ETA: 13s - loss: 0.3159 - acc: 0.87 - ETA: 12s - loss: 0.3218 - acc: 0.86 - ETA: 12s - loss: 0.3341 - acc: 0.85 - ETA: 12s - loss: 0.3390 - acc: 0.85 - ETA: 12s - loss: 0.3343 - acc: 0.85 - ETA: 12s - loss: 0.3299 - acc: 0.85 - ETA: 12s - loss: 0.3342 - acc: 0.85 - ETA: 11s - loss: 0.3292 - acc: 0.85 - ETA: 11s - loss: 0.3309 - acc: 0.86 - ETA: 11s - loss: 0.3350 - acc: 0.85 - ETA: 11s - loss: 0.3314 - acc: 0.85 - ETA: 11s - loss: 0.3346 - acc: 0.85 - ETA: 11s - loss: 0.3338 - acc: 0.85 - ETA: 10s - loss: 0.3317 - acc: 0.85 - ETA: 10s - loss: 0.3350 - acc: 0.85 - ETA: 10s - loss: 0.3386 - acc: 0.85 - ETA: 10s - loss: 0.3328 - acc: 0.85 - ETA: 10s - loss: 0.3354 - acc: 0.85 - ETA: 10s - loss: 0.3358 - acc: 0.85 - ETA: 9s - loss: 0.3325 - acc: 0.8610 - ETA: 9s - loss: 0.3293 - acc: 0.861 - ETA: 9s - loss: 0.3260 - acc: 0.864 - ETA: 9s - loss: 0.3236 - acc: 0.864 - ETA: 9s - loss: 0.3222 - acc: 0.864 - ETA: 9s - loss: 0.3206 - acc: 0.863 - ETA: 8s - loss: 0.3228 - acc: 0.862 - ETA: 8s - loss: 0.3195 - acc: 0.865 - ETA: 8s - loss: 0.3180 - acc: 0.866 - ETA: 8s - loss: 0.3202 - acc: 0.865 - ETA: 8s - loss: 0.3251 - acc: 0.862 - ETA: 8s - loss: 0.3263 - acc: 0.863 - ETA: 7s - loss: 0.3291 - acc: 0.863 - ETA: 7s - loss: 0.3317 - acc: 0.862 - ETA: 7s - loss: 0.3318 - acc: 0.861 - ETA: 7s - loss: 0.3317 - acc: 0.861 - ETA: 7s - loss: 0.3333 - acc: 0.861 - ETA: 7s - loss: 0.3331 - acc: 0.861 - ETA: 6s - loss: 0.3385 - acc: 0.860 - ETA: 6s - loss: 0.3356 - acc: 0.861 - ETA: 6s - loss: 0.3317 - acc: 0.863 - ETA: 6s - loss: 0.3304 - acc: 0.864 - ETA: 6s - loss: 0.3305 - acc: 0.864 - ETA: 6s - loss: 0.3317 - acc: 0.864 - ETA: 5s - loss: 0.3385 - acc: 0.863 - ETA: 5s - loss: 0.3374 - acc: 0.863 - ETA: 5s - loss: 0.3413 - acc: 0.860 - ETA: 5s - loss: 0.3447 - acc: 0.858 - ETA: 5s - loss: 0.3441 - acc: 0.859 - ETA: 5s - loss: 0.3433 - acc: 0.860 - ETA: 4s - loss: 0.3496 - acc: 0.857 - ETA: 4s - loss: 0.3514 - acc: 0.855 - ETA: 4s - loss: 0.3515 - acc: 0.855 - ETA: 4s - loss: 0.3534 - acc: 0.855 - ETA: 4s - loss: 0.3551 - acc: 0.855 - ETA: 4s - loss: 0.3555 - acc: 0.854 - ETA: 3s - loss: 0.3588 - acc: 0.853 - ETA: 3s - loss: 0.3576 - acc: 0.853 - ETA: 3s - loss: 0.3565 - acc: 0.855 - ETA: 3s - loss: 0.3551 - acc: 0.855 - ETA: 3s - loss: 0.3542 - acc: 0.856 - ETA: 3s - loss: 0.3524 - acc: 0.856 - ETA: 2s - loss: 0.3525 - acc: 0.857 - ETA: 2s - loss: 0.3503 - acc: 0.857 - ETA: 2s - loss: 0.3517 - acc: 0.856 - ETA: 2s - loss: 0.3505 - acc: 0.856 - ETA: 2s - loss: 0.3542 - acc: 0.855 - ETA: 2s - loss: 0.3555 - acc: 0.854 - ETA: 1s - loss: 0.3548 - acc: 0.854 - ETA: 1s - loss: 0.3572 - acc: 0.855 - ETA: 1s - loss: 0.3569 - acc: 0.855 - ETA: 1s - loss: 0.3556 - acc: 0.856 - ETA: 1s - loss: 0.3571 - acc: 0.854 - ETA: 1s - loss: 0.3588 - acc: 0.854 - ETA: 0s - loss: 0.3578 - acc: 0.854 - ETA: 0s - loss: 0.3565 - acc: 0.855 - ETA: 0s - loss: 0.3565 - acc: 0.855 - ETA: 0s - loss: 0.3590 - acc: 0.855 - ETA: 0s - loss: 0.3605 - acc: 0.855 - 17s 9ms/step - loss: 0.3600 - acc: 0.8550 - val_loss: 1.4817 - val_acc: 0.6133\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99151\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - ETA: 15s - loss: 0.2267 - acc: 0.90 - ETA: 16s - loss: 0.3849 - acc: 0.85 - ETA: 15s - loss: 0.3706 - acc: 0.85 - ETA: 15s - loss: 0.3557 - acc: 0.83 - ETA: 15s - loss: 0.3353 - acc: 0.86 - ETA: 15s - loss: 0.3108 - acc: 0.87 - ETA: 15s - loss: 0.2844 - acc: 0.89 - ETA: 15s - loss: 0.2953 - acc: 0.88 - ETA: 15s - loss: 0.2961 - acc: 0.87 - ETA: 14s - loss: 0.3333 - acc: 0.86 - ETA: 14s - loss: 0.3452 - acc: 0.86 - ETA: 14s - loss: 0.3299 - acc: 0.87 - ETA: 14s - loss: 0.3264 - acc: 0.87 - ETA: 14s - loss: 0.3175 - acc: 0.87 - ETA: 14s - loss: 0.3301 - acc: 0.87 - ETA: 13s - loss: 0.3338 - acc: 0.87 - ETA: 13s - loss: 0.3288 - acc: 0.87 - ETA: 13s - loss: 0.3204 - acc: 0.88 - ETA: 13s - loss: 0.3239 - acc: 0.87 - ETA: 13s - loss: 0.3246 - acc: 0.87 - ETA: 13s - loss: 0.3235 - acc: 0.87 - ETA: 12s - loss: 0.3165 - acc: 0.87 - ETA: 12s - loss: 0.3294 - acc: 0.87 - ETA: 12s - loss: 0.3241 - acc: 0.87 - ETA: 12s - loss: 0.3191 - acc: 0.88 - ETA: 12s - loss: 0.3147 - acc: 0.88 - ETA: 12s - loss: 0.3206 - acc: 0.87 - ETA: 11s - loss: 0.3174 - acc: 0.88 - ETA: 11s - loss: 0.3120 - acc: 0.88 - ETA: 11s - loss: 0.3137 - acc: 0.88 - ETA: 11s - loss: 0.3188 - acc: 0.87 - ETA: 11s - loss: 0.3214 - acc: 0.87 - ETA: 11s - loss: 0.3255 - acc: 0.87 - ETA: 10s - loss: 0.3229 - acc: 0.87 - ETA: 10s - loss: 0.3206 - acc: 0.88 - ETA: 10s - loss: 0.3191 - acc: 0.88 - ETA: 10s - loss: 0.3236 - acc: 0.87 - ETA: 10s - loss: 0.3192 - acc: 0.88 - ETA: 10s - loss: 0.3231 - acc: 0.87 - ETA: 9s - loss: 0.3200 - acc: 0.8788 - ETA: 9s - loss: 0.3207 - acc: 0.878 - ETA: 9s - loss: 0.3176 - acc: 0.879 - ETA: 9s - loss: 0.3165 - acc: 0.880 - ETA: 9s - loss: 0.3187 - acc: 0.877 - ETA: 9s - loss: 0.3177 - acc: 0.876 - ETA: 8s - loss: 0.3241 - acc: 0.873 - ETA: 8s - loss: 0.3254 - acc: 0.874 - ETA: 8s - loss: 0.3298 - acc: 0.870 - ETA: 8s - loss: 0.3289 - acc: 0.870 - ETA: 8s - loss: 0.3274 - acc: 0.871 - ETA: 8s - loss: 0.3243 - acc: 0.871 - ETA: 7s - loss: 0.3208 - acc: 0.874 - ETA: 7s - loss: 0.3183 - acc: 0.875 - ETA: 7s - loss: 0.3183 - acc: 0.875 - ETA: 7s - loss: 0.3142 - acc: 0.878 - ETA: 7s - loss: 0.3134 - acc: 0.878 - ETA: 7s - loss: 0.3114 - acc: 0.878 - ETA: 6s - loss: 0.3154 - acc: 0.878 - ETA: 6s - loss: 0.3136 - acc: 0.879 - ETA: 6s - loss: 0.3174 - acc: 0.880 - ETA: 6s - loss: 0.3163 - acc: 0.880 - ETA: 6s - loss: 0.3133 - acc: 0.882 - ETA: 6s - loss: 0.3100 - acc: 0.884 - ETA: 5s - loss: 0.3137 - acc: 0.882 - ETA: 5s - loss: 0.3125 - acc: 0.882 - ETA: 5s - loss: 0.3103 - acc: 0.883 - ETA: 5s - loss: 0.3070 - acc: 0.884 - ETA: 5s - loss: 0.3095 - acc: 0.883 - ETA: 5s - loss: 0.3145 - acc: 0.880 - ETA: 4s - loss: 0.3169 - acc: 0.879 - ETA: 4s - loss: 0.3211 - acc: 0.878 - ETA: 4s - loss: 0.3210 - acc: 0.879 - ETA: 4s - loss: 0.3216 - acc: 0.879 - ETA: 4s - loss: 0.3269 - acc: 0.879 - ETA: 4s - loss: 0.3279 - acc: 0.878 - ETA: 3s - loss: 0.3266 - acc: 0.878 - ETA: 3s - loss: 0.3263 - acc: 0.878 - ETA: 3s - loss: 0.3252 - acc: 0.878 - ETA: 3s - loss: 0.3239 - acc: 0.879 - ETA: 3s - loss: 0.3229 - acc: 0.879 - ETA: 3s - loss: 0.3271 - acc: 0.876 - ETA: 2s - loss: 0.3263 - acc: 0.877 - ETA: 2s - loss: 0.3256 - acc: 0.877 - ETA: 2s - loss: 0.3240 - acc: 0.878 - ETA: 2s - loss: 0.3256 - acc: 0.877 - ETA: 2s - loss: 0.3273 - acc: 0.876 - ETA: 2s - loss: 0.3265 - acc: 0.876 - ETA: 1s - loss: 0.3252 - acc: 0.877 - ETA: 1s - loss: 0.3280 - acc: 0.877 - ETA: 1s - loss: 0.3309 - acc: 0.876 - ETA: 1s - loss: 0.3304 - acc: 0.877 - ETA: 1s - loss: 0.3280 - acc: 0.878 - ETA: 1s - loss: 0.3281 - acc: 0.878 - ETA: 0s - loss: 0.3274 - acc: 0.878 - ETA: 0s - loss: 0.3276 - acc: 0.877 - ETA: 0s - loss: 0.3276 - acc: 0.877 - ETA: 0s - loss: 0.3282 - acc: 0.876 - ETA: 0s - loss: 0.3274 - acc: 0.876 - ETA: 0s - loss: 0.3286 - acc: 0.875 - 17s 9ms/step - loss: 0.3286 - acc: 0.8760 - val_loss: 1.9047 - val_acc: 0.6467\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fb5880bac8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "epochs = 10\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unsupported format character '%' (0x25) at index 18",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bbb2992407fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Test accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskin_cancer_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskin_cancer_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy: %.4%%'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: unsupported format character '%' (0x25) at index 18"
     ]
    }
   ],
   "source": [
    "# get index of predicted skin for each image in test set\n",
    "skin_cancer_predictions = [np.argmax(model.predict(np.expand_dims(test_tensors[0], axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# Test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(skin_cancer_predictions)==np.argmax(test_targets, axis=1))/len(skin_cancer_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = path_to_tensor('test_images\\\\2.jpg').astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 65.500%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.3f%%' %test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"Step5\"></a>\n",
    "## Step 5: Algorithm To Predict Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictCancer(imgpath):\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    imgpath: string\n",
    "            Contains the address of image.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = path_to_tensor(imgpath).astype('float32')/255\n",
    "    result = model.predict(data)\n",
    "    print(result)\n",
    "    print(class_names[np.argmax(result)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter image path\n",
      "2\n",
      "[[0.27490517 0.681469   0.04362569]]\n",
      "nevus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.8864327e-18, 1.0000000e+00, 3.2709350e-26]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = input(\"Enter image path\\n\")\n",
    "predictCancer(\"test_images\\\\\" + path + \".jpg\")\n",
    "model.predict(path_to_tensor('test_images\\\\4.jpg').astype('float32')/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test Image](test_images\\\\2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
